# LLM Context f√ºr Disce / Kleiner B√§r

> **Zweck:** Schnelleinstieg f√ºr Claude, Goose oder andere LLMs, die mit diesem Repo arbeiten.  
> **Stand:** Januar 2026  
> **Maintainer:** [Dein Name]

---

## Was ist das hier?

**Disce** ist ein Sprachcoaching-System f√ºr fortgeschrittene Deutschlernende (B2‚ÄìC2).

Das System besteht aus zwei Hauptmodulen:

| Modul | Rolle | Hauptdateien |
|-------|-------|--------------|
| **Gro√üer B√§r** | Speaking Coach ‚Äì UI, Feedback-Loop, Session-Management | `pages/grosser_baer.py`, `grosser_baer/*.py` |
| **Kleiner B√§r** | Feature-Extraktion ‚Äì NLP, CAF-Metriken, CEFR-Sch√§tzung | `features_viewer.py`, `disce_core.py` |

**Kern-Idee:** User spricht/schreibt ‚Üí Kleiner B√§r extrahiert Metriken ‚Üí LLM generiert narratives Feedback ‚Üí User reflektiert.

---

## Aktueller Stand (Januar 2026)

### ‚úÖ Funktioniert produktiv
- **Deterministische Analyse (Schicht 1):** 30+ NLP-Features (SoMaJo, HanTa, spaCy, LanguageTool, wordfreq)
- **CEFR-Sch√§tzung:** Regelbasiert aus 8 Dimensionen
- **LLM-Feedback:** GPT-4o-mini mit strukturiertem Prompt
- **Session-Logging:** Airtable via Make Webhook
- **Streamlit-UI:** Vollst√§ndiger Flow (Login ‚Üí Pretest ‚Üí Task ‚Üí Aufnahme ‚Üí Feedback ‚Üí Reflexion)

### üîß Funktioniert bedingt
- **Audio-Aufnahme:** Browser-Mikrofon via `audio_recorder_streamlit`
- **Transkription:** OpenAI Whisper (wenn API-Key vorhanden), sonst Mock-Modus

### ‚ùå Noch nicht implementiert
- **Azure Speech-to-Text:** Vorbereitet in Architektur, Code fehlt
- **Azure Pronunciation Assessment:** Geplant f√ºr Prosodie-Analyse
- **Schicht 2 (Azure Services):** Komplett ausstehend

### ‚ö†Ô∏è Bekanntes Tech Debt
- `features_viewer.py` hat 1.865 Zeilen ‚Üí sollte aufgeteilt werden
- 4 von 5 Home-KPIs sind Hardcoded-Defaults (nur `sentence_cohesion` ist echt)
- Demo-Dateien im Root-Verzeichnis ‚Üí sollten in `/experiments/` verschoben werden
- "Neuer Ordner" existiert leer im Repo

---

## Wo finde ich was?

### Dokumentation
| Datei | Inhalt |
|-------|--------|
| `docs/architecture.md` | Systemarchitektur, Datenfluss, 4 Schichten |
| `docs/llm_context.md` | **Diese Datei** ‚Äì Schnelleinstieg |
| `docs/generated/repo_map.md` | Auto-generierte Dateistruktur |
| `docs/generated/modules.md` | Auto-generierte Modul-√úbersicht |
| `docs/generated/integrations.md` | Erkannte externe Services |

### Code-Einstiegspunkte
| Was | Wo |
|-----|-----|
| **Streamlit Entry** | `app.py` |
| **Speaking Coach UI** | `pages/grosser_baer.py` |
| **Feature-Extraktion** | `features_viewer.py` |
| **Features ‚Üí UI Bridge** | `disce_core.py` |
| **LLM Prompts** | `grosser_baer/prompts.py` |
| **Task-Templates** | `grosser_baer/task_templates.py` |
| **OpenAI Services** | `openai_services.py` |
| **Pretest/MASQ** | `config/pretest_loader.py`, `config/pretest_config.json` |

### Datenfluss (vereinfacht)
```
User Input (Audio/Text)
       ‚Üì
[Whisper] ‚Üí Transkript
       ‚Üì
[features_viewer.py] ‚Üí 30+ NLP-Features
       ‚Üì
[disce_core.py] ‚Üí Aggregation (8 Dimensionen, CEFR, KPIs, Hotspots)
       ‚Üì
[grosser_baer.py] ‚Üí build_coach_input() ‚Üí JSON-Block
       ‚Üì
[openai_services.py] ‚Üí GPT-4o-mini mit SYSTEM_PROMPT_COACH
       ‚Üì
Feedback-Anzeige + Reflexion + Airtable-Logging
```

---

## F√ºr LLMs: Do's and Don'ts

### ‚úÖ Do's
- **Kleine Schritte:** √Ñnderungen fokussiert und testbar halten
- **Metriken nutzen:** Das System ist datenbasiert ‚Äì Feedback sollte auf echten Features basieren
- **Docs aktualisieren:** Nach strukturellen √Ñnderungen `python generate_docs.py` ausf√ºhren
- **Kontext beachten:** Pretest-Daten (CEFR-Selbsteinsch√§tzung, MASQ) flie√üen ins Coaching ein
- **Schichten respektieren:** Deterministisch (Schicht 1) ‚Üí Azure (Schicht 2) ‚Üí LLM (Schicht 3) ‚Üí Interpretation (Schicht 4)

### ‚ùå Don'ts
- **Nicht `features_viewer.py` komplett umschreiben** ohne vorherigen Refactoring-Plan
- **Keine neuen Integrationen** ohne Eintrag in Docs
- **Keine Magic Numbers** ‚Äì Schwellenwerte dokumentieren oder in Config auslagern
- **Nicht Azure/Whisper voraussetzen** ‚Äì Mock-Modus muss immer funktionieren

---

## Typische Aufgaben f√ºr LLMs

### 1. Feature hinzuf√ºgen
1. Funktion in `features_viewer.py` implementieren
2. In `analyze_all()` aufrufen
3. In `compute_dimension_scores()` einbinden (falls relevant f√ºr Dimensionen)
4. In `build_metrics_summary()` f√ºr LLM-Output aufnehmen

### 2. Neuen Task-Typ erstellen
1. Template in `grosser_baer/task_templates.py` hinzuf√ºgen
2. Evaluation-Focus und Meta-Prompts definieren
3. Ggf. spezifische Prompt-Anpassungen in `prompts.py`

### 3. CEFR-Sch√§tzung verbessern
- Schwellenwerte in `estimate_cefr_score_from_dims()` anpassen
- Gewichtungen der Dimensionen in `compute_dimension_scores()` pr√ºfen

### 4. Azure-Integration bauen
- Ziel: `layer2_azure` in `coach_input["analysis"]` bef√ºllen
- Pronunciation Assessment Scores integrieren
- Prosodie-Daten (Pitch, Tempo) extrahieren

---

## Schnellstart: Lokale Entwicklung

```bash
# Repo klonen / updaten
cd /pfad/zu/Kleiner-Baer
git pull origin main

# Virtuelle Umgebung (falls nicht vorhanden)
python -m venv venv
source venv/bin/activate  # oder venv\Scripts\activate auf Windows

# Dependencies
pip install -r requirements.txt
python -m spacy download de_core_news_lg

# Streamlit starten
streamlit run app.py

# Generated Docs aktualisieren
python generate_docs.py
```

---

## Kontakt / Fragen

Bei Unklarheiten:
1. Pr√ºfe `docs/architecture.md` f√ºr Systemverst√§ndnis
2. Pr√ºfe `docs/generated/modules.md` f√ºr Code-Struktur
3. Frage den Maintainer

---

*Zuletzt aktualisiert: 2026-01-24*
