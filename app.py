import streamlit as st
from disce_core import analyze_text_for_ui

st.set_page_config(page_title="Disce CEFR-Demo", layout="wide")

st.title("Disce ‚Äì CEFR-Demo f√ºr Schreibkompetenz")
st.write("Gib einen deutschen Text ein und erhalte eine grobe Niveausch√§tzung (MERLIN-basiert).")

# üîß Debug-Schalter in der Sidebar
st.sidebar.header("Debug")
debug_mode = st.sidebar.checkbox("Debug-Modus aktivieren", value=False)

# Eingabe
default_text = "Schreibe hier deinen deutschen Text rein..."
text = st.text_area("Text eingeben", value=default_text, height=300)

if st.button("Analysieren"):
    if not text.strip():
        st.warning("Bitte zuerst einen Text eingeben.")
    else:
        with st.spinner("Analysiere Text..."):
            result = analyze_text_for_ui(text)

        # Hauptergebnis anzeigen
        st.success(f"Fertig! Gesch√§tztes Niveau: **{result['cefr_label']}** "
                   f"(Score: {result['cefr_score']:.2f})")

        # Spaltenlayout
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("Grunddaten")
            st.write(f"- S√§tze: `{result['num_sentences']}`")
            st.write(f"- Tokens: `{result['num_tokens']}`")

            st.subheader("Dimensionen (0‚Äì1)")
            dims = result["dims"]
            for name, val in dims.items():
                if name == "written_formality":
                    continue
                st.write(f"- **{name}**: {val:.3f}")

        with col2:
            st.subheader("Lesbarkeit (LIX)")
            lix = result["lix"]
            if lix is not None:
                st.write(f"- LIX: `{lix['lix']:.1f}`")
                st.write(f"- Lange W√∂rter (>=7): `{lix['num_long_words']}` "
                         f"({lix['share_long_words']:.2f} Anteil)")
            else:
                st.write("- LIX: nicht berechenbar")

            st.subheader("Lexik")
            lex = result["lex_feats"]
            st.write(f"- Unikate Wortformen: `{lex['unique_tokens']}`")
            st.write(f"- Unikate Lemmata: `{lex['unique_lemmas']}`")
            st.write(f"- TTR: `{lex['ttr']:.3f}`")
            st.write(f"- Lemma-TTR: `{lex['lemma_ttr']:.3f}`")

            # Wortfrequenz-Sektion
            st.subheader("Wortfrequenz (SUBTLEX-DE)")
            freq = result["freq_feats"]
            st.write(f"- √ò Zipf-Frequenz: `{freq['avg_zipf']:.2f}`")
            st.write(f"- Seltene W√∂rter (Zipf<3): `{freq['rare_word_count']}` ({freq['rare_word_share']:.1%})")
            st.write(f"- Sehr h√§ufige (Zipf>5.5): `{freq['very_common_share']:.1%}`")
            st.write(f"- Schwierigkeitsscore: `{freq['difficulty_score']:.3f}`")

            # Seltene W√∂rter anzeigen
            rare_words = result.get("rare_words", [])
            if rare_words:
                with st.expander("üîç Seltenste W√∂rter im Text"):
                    for w in rare_words[:10]:
                        st.write(f"- **{w['word']}** ({w['lemma']}, Zipf={w['zipf']})")

        st.subheader("Kommentar zur Sch√§tzung")
        st.write(
            "Diese Sch√§tzung basiert auf einem Regressionsmodell, das auf dem MERLIN-Korpus "
            "kalibriert wurde (B1‚ÄìC1-Lernertexte). Die Grammatikdimension wird aktuell "
            "nur diagnostisch berechnet, flie√üt aber **noch nicht** in den CEFR-Score ein."
        )

        # üîß DEBUG-BEREICH (NUR wenn debug_mode UND result existiert)
        if debug_mode:
            st.markdown("---")
            st.subheader("üîß Debug-Ansicht ‚Äì Roh-Features")

            tab1, tab2, tab3, tab4 = st.tabs(
                ["Grammatik & Dimensionen", "Lexik & Wortfrequenz", "Koh√§sion & Referenzen", "Struktur & Satztypen"]
            )

            with tab1:
                st.markdown("**Grammatik (LanguageTool)**")
                st.write(f"- Issues gesamt: `{result['num_issues']}`")
                st.write(f"- Fehler pro 100 Tokens: `{result['errors_per_100']:.2f}`")

                st.markdown("**Normalisierte Dimensionen (0‚Äì1)**")
                for name, val in result["dims"].items():
                    st.write(f"- `{name}`: **{val:.3f}**")

            with tab2:
                st.markdown("**Lexikalische Basiswerte**")
                lex = result["lex_feats"]
                st.write(f"- Unikate Wortformen: `{lex['unique_tokens']}`")
                st.write(f"- Unikate Lemmata: `{lex['unique_lemmas']}`")
                st.write(f"- TTR: `{lex['ttr']:.3f}`")
                st.write(f"- Lemma-TTR: `{lex['lemma_ttr']:.3f}`")
                st.write(f"- Anteil Inhaltsw√∂rter: `{lex['content_word_share']:.3f}`")

                st.markdown("**Wortfrequenz (wordfreq)**")
                freq = result["freq_feats"]
                st.write(f"- √ò Zipf-Frequenz: `{freq['avg_zipf']:.2f}`")
                st.write(f"- Median Zipf: `{freq['median_zipf']:.2f}`")
                st.write(f"- Min/Max Zipf: `{freq['min_zipf']:.2f}` / `{freq['max_zipf']:.2f}`")
                st.write(f"- Seltene W√∂rter (Zipf<3): `{freq['rare_word_count']}` ({freq['rare_word_share']:.1%})")
                st.write(f"- Sehr h√§ufige (Zipf>5.5): `{freq['very_common_share']:.1%}`")
                st.write(f"- Unbekannte W√∂rter: `{freq['unknown_count']}` ({freq['unknown_share']:.1%})")
                st.write(f"- Schwierigkeitsscore: `{freq['difficulty_score']:.3f}`")

                rare_words = result.get("rare_words", [])
                if rare_words:
                    st.markdown("**Seltenste W√∂rter:**")
                    st.table(rare_words[:20])

                st.markdown("**Dependency-Baumtiefe (spaCy)**")
                dep = result.get("dep_tree")
                if dep and dep.get("num_sents_parsed", 0) > 0:
                    st.write(f"- √ò Baumtiefe pro Satz: `{dep['avg_tree_depth']:.2f}`")
                    st.write(f"- Min/Max Baumtiefe: `{dep['min_tree_depth']}` / `{dep['max_tree_depth']}`")
                    st.write(f"- S√§tze (spaCy): `{dep['num_sents_parsed']}`")
                else:
                    st.write("_Keine Daten (spaCy nicht geladen oder Fehler)._")

            with tab3:
                st.markdown("**Konnektoren**")
                coh = result["coh_feats"]
                st.write(f"- Konnektoren gesamt: `{coh['connector_count']}`")
                st.write(f"- Verschiedene Konnektoren: `{coh['connector_type_count']}`")
                st.write(f"- Dichte (pro 100 Tokens): `{coh['connector_density_per_100_tokens']:.2f}`")
                if coh["connectors_used"]:
                    st.write("Verwendete Konnektoren: " + ", ".join(coh["connectors_used"]))

                st.markdown("**Lexikalische Wiederaufnahme (Overlap)**")
                overlap = result["overlap"]
                if overlap:
                    st.write(f"- √ò Overlap benachbarter S√§tze: `{overlap['avg_overlap']:.3f}`")
                    st.write(f"- Min/Max Overlap: `{overlap['min_overlap']:.3f}` / `{overlap['max_overlap']:.3f}`")
                    st.write(f"- Satzpaare: `{overlap['num_pairs']}`")
                else:
                    st.write("_Zu wenig Daten f√ºr Overlap._")

                st.markdown("**Pronomen & Referenzen**")
                pron = result["pronouns"]
                st.write(f"- Pronomen gesamt: `{pron['total_pronouns']}`")
                st.write(f"- Anteil Pronomen: `{pron['share_pronouns']:.3f}`")
                st.write(f"- 3.-Person-Referenzen: `{pron['third_person_refs']}`")
                st.json(pron["by_person"])

            with tab4:
                st.markdown("**Satztypen**")
                st.json(result["sent_types"])

                st.markdown("**Absatzstruktur**")
                st.json(result["para_info"])

                st.markdown("**Direkte Rede**")
                st.json(result["direct_speech"])

                st.markdown("**Interpunktion**")
                st.json(result["punct_feats"])

                st.markdown("**Modalpartikeln**")
                st.json(result["mp_feats"])
